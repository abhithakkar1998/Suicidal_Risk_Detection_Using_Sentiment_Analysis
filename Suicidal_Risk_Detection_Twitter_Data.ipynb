{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import re, string, random\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "\n",
    "\n",
    "#Global Database to store words their polarity score and frequencies\n",
    "word_freq = pd.DataFrame(columns=['word','score','freq'])\n",
    "\n",
    "#Consumer Key for Twitter API access\n",
    "consumer_key = \"wvSK124NAIimBMX0aY9gJ1O9H\" \n",
    "consumer_secret = \"lTlgZPfSSUQk2ZZkORex1asQzTL7KqOlgOJd6fj3BBTmVshoTq\"\n",
    "access_key = \"1074517762960052224-m7DqeZhrjH77nKX3cJMzMTMTDRoIuy\"\n",
    "access_secret = \"F6l0PtL4ub5WYu4YpT29MBKQbIRy9bqPmAXjKkFeK4q4i\"\n",
    "\n",
    "#Extract tweet from twitter API - Tweepy\n",
    "def get_tweets(username): \n",
    "          \n",
    "        # Authorization to consumer key and consumer secret \n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "  \n",
    "        # Access to user's access key and access secret \n",
    "        auth.set_access_token(access_key, access_secret) \n",
    "  \n",
    "        # Calling api \n",
    "        api = tweepy.API(auth) \n",
    "  \n",
    "        # 10 tweets to be extracted \n",
    "        number_of_tweets=10\n",
    "        tweets = api.user_timeline(screen_name=username) \n",
    "  \n",
    "        # Empty Array \n",
    "        tmp=[]  \n",
    "  \n",
    "        # create array of tweet information: username,  \n",
    "        # tweet id, date/time, text \n",
    "        tweets_for_csv = [tweet.text for tweet in tweets]\n",
    "        return tweets_for_csv\n",
    "\n",
    "#Data preprocessing for Naive Bayes Classifier\n",
    "#removing unwanted data\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    #empty list for cleaned tokens\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    #check for token and tag in the tweet_tokens\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        \n",
    "        #remove @<string> and URL\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "        \n",
    "        #Add part of speech tag\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        #Lemmatizing the tokens according to words present in WordNet corpus \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        #If token is not a punctuation and not in stop words add it to list\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "            \n",
    "    return cleaned_tokens\n",
    "\n",
    "#creating a datamodel for Naive-Bayes Classifier\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    \n",
    "    #Create a dictionary with token as key and \"True\" as its value\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "    \n",
    "#Preprocessing data for sentiment score calculation using SentiWordNet\n",
    "#Removing the unwanted data\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"<br />\", \" \")\n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', text)\n",
    "    text = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", text)\n",
    "    text = re.sub(\"#\",\"\",text)\n",
    " \n",
    "    return text\n",
    "\n",
    "#Convert between the PennTreebank tags to simple Wordnet tags\n",
    "def penn_to_wn(tag):\n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "#Storing the scores and frequency of words according to sentiwordnet\n",
    "def sentiment_scores(text): \n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    tokens_count = 0\n",
    "    global word_freq\n",
    "    \n",
    "    text = clean_text(text)\n",
    " \n",
    " \n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    \n",
    "    for raw_sentence in raw_sentences:\n",
    "        \n",
    "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
    "        print(tagged_sentence)\n",
    "\n",
    "        for word, tag in tagged_sentence:\n",
    "            \n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            print(\"Word = \",word)\n",
    "            print(\"Tag = \",wn_tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "                continue\n",
    "                \n",
    "            word = stemmer.stem(word)\n",
    "            print(word)\n",
    "            \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            print(lemma)\n",
    "            \n",
    "            if not lemma:\n",
    "                continue\n",
    "\n",
    "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "            if not synsets:\n",
    "                continue\n",
    "\n",
    "            # Take the first sense, the most common\n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    "            print(\"SWN = \",swn_synset)\n",
    "\n",
    "            if swn_synset.pos_score() < swn_synset.neg_score():\n",
    "                if word not in word_freq.values:\n",
    "                    word_freq = word_freq.append(pd.DataFrame({\"word\":[word],\"score\":[swn_synset.neg_score()],\"freq\":[1]}), ignore_index = True)\n",
    "\n",
    "                else:\n",
    "                    word_freq.loc[word_freq['word']==word,'freq'] +=1\n",
    "\n",
    "            tokens_count += 1\n",
    "            \n",
    "    if not tokens_count:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Datasample for training of Naive Bayes classifier\n",
    "    positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "    negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "    \n",
    "    #Selection of english stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    #Tokenizing the data\n",
    "    positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "    negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "    positive_cleaned_tokens_list = []\n",
    "    negative_cleaned_tokens_list = []\n",
    "\n",
    "    #removing noise from the token lists\n",
    "    for tokens in positive_tweet_tokens:\n",
    "        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    for tokens in negative_tweet_tokens:\n",
    "        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    #Create a model for classifier in form of a dictionary with each token as key and value as true\n",
    "    #for both negative and positive token list\n",
    "    positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "    negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "    #Create dataset with each token list dictionary and adding positive for element in positive model\n",
    "    #and negative for element in negative model\n",
    "    positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                         for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "    negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                         for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "    dataset = positive_dataset + negative_dataset\n",
    "\n",
    "    #randomly shuffles the dataset each time executed\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    #dividing the dataset into training and testing\n",
    "    train_data = dataset[:7000]\n",
    "    test_data = dataset[7000:]\n",
    "    \n",
    "    #Training the classifier and checking the accuracy score\n",
    "    classifier = NaiveBayesClassifier.train(train_data)\n",
    "    print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "    \n",
    "    #Exctracting tweets of a user\n",
    "    tweets = get_tweets(\"@realDonaldTrump\")\n",
    "    \n",
    "    #Checking for each tweet\n",
    "    for custom_tweet in tweets:\n",
    "        \n",
    "        #Removing noise\n",
    "        custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "        \n",
    "        #use Naive Bayes classifier\n",
    "        sentiment = classifier.classify(dict([token, True] for token in custom_tokens))\n",
    "        \n",
    "        #If sentiment is negative classify using SentiWordNet\n",
    "        if(sentiment == 'Negative'):\n",
    "            \n",
    "            #Storing data for custom tweet\n",
    "            sentiment_scores(custom_tweet)\n",
    "            \n",
    "            sentiment_freq = 0.0\n",
    "            total_word_freq = 0.0\n",
    "            word_freq.dropna()\n",
    "            \n",
    "            #Using mathematical equation to calculate overall score.\n",
    "            for word in word_freq.word:\n",
    "                sentiment_freq += (word_freq.loc[word_freq['word']==word,'score'].item() * word_freq.loc[word_freq['word']==word,'freq']).item()\n",
    "                total_word_freq += word_freq.loc[word_freq['word']==word,'freq'].item()\n",
    "\n",
    "            if sentiment_freq == 0 or total_word_freq == 0:\n",
    "                print(\"neutral\")\n",
    "            else:    \n",
    "                sentiment_score = sentiment_freq / total_word_freq\n",
    "\n",
    "                print(sentiment_score)\n",
    "\n",
    "                if sentiment_score >= 0.7:\n",
    "                    print(\"suicidal\")\n",
    "                elif sentiment_score < 0.7 and sentiment_score >= 0.5:\n",
    "                    print(\"depressed\")\n",
    "                else:\n",
    "                    print(\"neutral\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
